import pandas as pd# Load your summariesreplication_summary = pd.read_csv('per_dataset_clock_metrics.csv')paper_summary = pd.read_csv('test_summary.csv')# Merge both tables on 'dataset'comparison_df = pd.merge(    replication_summary,    paper_summary,    on="Dataset",    suffixes=("_replication", "_paper"),    how="inner")# Compute absolute differences for each metricmetrics = [    "MAE (Horvath2013)", "MSE (Horvath2013)", "R (Horvath2013)", "Median Error (Horvath2013)",    "MAE (Altumage)", "MSE (Altumage)", "R (Altumage)", "Median Error (Altumage)"]# Add difference columnsfor metric in metrics:    col_rep = f"{metric}_replication"    col_paper = f"{metric}_paper"    diff_col = f"{metric} Δ"    comparison_df[diff_col] = comparison_df[col_rep] - comparison_df[col_paper]# Create list of only the Δ columns + Datasetdiff_columns = ["Dataset"] + [f"{m} Δ" for m in metrics]# Save just the differences to CSVcomparison_df[diff_columns].to_csv("replication_vs_paper_differences.csv", index=False)import matplotlib.pyplot as pltimport seaborn as snsfrom scipy.interpolate import make_interp_splineimport numpy as np# Prepare data for plottingplot_df = pd.DataFrame()for metric in metrics:    temp_df = comparison_df[["Dataset", f"{metric}_replication", f"{metric}_paper"]].copy()    temp_df = temp_df.melt(id_vars="Dataset",                           value_vars=[f"{metric}_replication", f"{metric}_paper"],                           var_name="Source", value_name="Value")    temp_df["Metric"] = metric    temp_df["Source"] = temp_df["Source"].str.extract(r'_(replication|paper)')    plot_df = pd.concat([plot_df, temp_df], ignore_index=True)# Plotting: One plot per metric with spline overlaysns.set(style="whitegrid")datasets = comparison_df["Dataset"].tolist()dataset_idx = np.arange(len(datasets))for metric in metrics:    x = comparison_df[f"{metric}_paper"]    y = comparison_df[f"{metric}_replication"]    plt.figure(figsize=(6, 6))    plt.scatter(x, y, alpha=0.7)    # Add 45-degree reference line    max_val = max(x.max(), y.max())    min_val = min(x.min(), y.min())    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Agreement')    plt.title(f"Scatterplot: {metric}")    plt.xlabel("Paper")    plt.ylabel("Replication")    plt.grid(True)    plt.legend()    plt.tight_layout()    plt.show()print("\nSummary of Differences Between Replication and Paper:\n")summary_rows = []for metric in metrics:    col_rep = f"{metric}_replication"    col_paper = f"{metric}_paper"    diff = comparison_df[col_rep] - comparison_df[col_paper]    mae = diff.abs().mean()    bias = diff.mean()    relative_error = (diff / comparison_df[col_paper]).abs().mean() * 100    print(f"{metric}:")    print(f"  Mean Absolute Error (|replication - paper|): {mae:.4f}")    print(f"  Bias (replication - paper): {bias:.4f}")    print(f"  Mean Relative Error (%): {relative_error:.2f}%\n")    summary_rows.append({        "Metric": metric,        "MAE": mae,        "Bias": bias,        "Mean Relative Error (%)": relative_error    })print("\nMAE-based Normalized Accuracy Score:\n")for metric in metrics:    rep = comparison_df[f"{metric}_replication"]    paper = comparison_df[f"{metric}_paper"]    mae = (rep - paper).abs().mean()    mean_paper = paper.abs().mean()    # Avoid division by zero    accuracy = 1 - mae / (mean_paper + 1e-10000)    print(f"{metric}: Accuracy Score = {accuracy:.3f}")